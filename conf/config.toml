# prepare（必须）:
#   1、程序运行前，首先需要初始化程序数据表
#   2、配置 reverse 自定义转换规则
#   - 优先级：表字段类型 > 库字段类型 两者都没配置默认采用内置转换规则
# reverse:
#   1、prepare 前提必须阶段
#   2、根据内置表结构转换规则或者手工配置表结构转换规则进行 schema 迁移
# gather:
#   1、用于收集评估 oracle -> mysql/tidb 迁移成本信息，只适用于 schema 级别
# check:
#   1、表结构检查(独立于表结构转换，可单独运行，校验规则使用内置规则)
# all:（全量 + 增量模式）
#   1、全量数据迁移
#   2、增量数据迁移
# full: (全量模式)
#   1、全量数据迁移 -> REPLACE INTO
# csv：（全量模式）
#   1、全量数据导出 -> CSV
[app]
# 事务 batch 数
# 用于数据写入 batch 提交事务数
insert-batch-size = 100
# 是否开启更新元数据 meta-schema 库表慢日志，单位毫秒
slowlog-threshold = 300
# 任务并发 -- 只用于 reverse/check 模式阶段
threads = 256
# pprof 端口
pprof-port = ":9696"

[diff]
chunk-size = 50000
# 检查数据并发数
diff-threads = 128
# 只检查数据行数
# 设置 true 代表只检查数据行数，设置 false 代表使用 checksum 数据对比以及输出对应差异数据
only-check-rows = false
# 断点续检，代表从上次 checkpoint 开始检查
enable-checkpoint = true
# 忽略表结构、collation 以及 character 检查，数据校验是否校验表结构，以上游表结构为准
ignore-struct-check = true
# 差异修复 SQL 文件, ONLY 用于下游数据库变更修复
fix-sql-file = "fix.sql"

# diff 某些表单独配置 -> 源端表
#[[table-config]]
# 源端表
#source-table = "marvin"
# 指定 NUMBER 类型字段，必须带索引
#index-fields = "id"
# 指定检查数据范围或者查询条件
# range 优先级高于 index-fields
#range = "age > 10 AND age< 20"

[csv]
# CSV 文件是否包含表头
header = true
# 字段分隔符，支持一个或多个字符，默认值为 ','
separator = '|#|'
# 行尾定界字符，支持一个或多个字符, 默认值 "\r\n" （回车+换行）
terminator = "|+|\r\n"
# 字符串引用定界符，支持一个或多个字符，设置为空表示字符串未加引号
delimiter = '"'
# 使用反斜杠 (\) 来转义导出文件中的特殊字符
escape-backslash = true
# 目标数据库字符集 utf8/gbk，设置为空表示以上游数据库为准
charset = "utf8"
# 1、任务行数数，固定动作，一旦确认，不能更改，除非设置 enable-checkpoint = false，重新导出导入
# 2、代表每张表每并发处理多少行数
# 3、代表多少行数据切分一个 csv 文件
# 4、建议是 insert-batch-size 整数倍
rows = 100000
# 数据文件输出目录, 所有表数据输出文件目录，需要磁盘空间充足
# 目录格式：/data1/${target_dbname}/${table_name}
output-dir = "/data1"
# 用于初始化表任务并发数【写下游 meta 数据库】
task-threads = 128
# 表导出导入并发数，同时处理多少张上游表，可动态变更
table-threads = 8
# 1、单表 SQL 执行并发数，表内并发，表示同时多少并发 SQL 读取上游表数据，可动态变更
# 2、单表 csv 并发写线程数，表示同时多少个 csv 文件同时写，可动态变更
sql-threads = 64
# 关于全量断点恢复
#   - 若想断点恢复，设置 enable-checkpoint = true,首次一旦运行则 chunk-size 数不能调整，
#   - 若不想断点恢复或者重新调整 chunk-size 数，设置 enable-checkpoint = false,重新运行全量任务
#   - 无法断点续传期间，则需要设置 enable-checkpoint = false 重新导入导出
enable-checkpoint = true

[full]
# 表间串行，表内并发
# 任务 chunk 数，固定动作，一旦确认，不能更改，除非设置 enable-checkpoint = false，重新导出导入
# 1、代表每张表每并发处理多少行数
# 2、建议参数值是 insert-batch-size 整数倍，会根据 insert-batch-size 大小切分
chunk-size = 100000
# 用于初始化表任务并发数【写下游 meta 数据库】
task-threads = 128
# 表导出导入并发数，同时处理多少张上游表，可动态变更
table-threads = 4
# 单表 SQL 执行并发数，表示同时多少并发 SQL 读取上游表数据，可动态变更
sql-threads = 32
# 每 sql-threads 线程写下游并发数，可动态变更
apply-threads = 64
# 关于全量断点恢复
#   - 若想断点恢复，设置 enable-checkpoint = true,首次一旦运行则 chunk-size 数不能调整，
#   - 若不想断点恢复或者重新调整 chunk-size 数，设置 enable-checkpoint = false,重新运行全量任务
#   - 无法断点续传期间，则需要设置 enable-checkpoint = false 重新导入导出
enable-checkpoint = true

[all]
# logminer 单次挖掘最长耗时，单位: 秒
logminer-query-timeout   = 300
# 并发筛选 oracle 日志数
filter-threads = 16
# 并发表应用数，同时处理多少张表
apply-threads = 4
# apply-threads 每个表并发处理最大工作对列
worker-queue = 128
# apply-threads 每个表并发处理最大任务分发数
worker-threads = 64

[source]
# 源端 oracle 连接串
# 若需要增量数据同步，则需要 logminer 权限
username = "c##logminer"
password = "logminer"
host = "172.16.4.94"
port = 1521
service-name = "orclpdb"
# oracle instance client dir
lib-dir = "/Users/marvin/storehouse/oracle/instantclient_19_8"
# 配置 oracle 连接参数
# 配置 oracle 连接会话 session 变量
connect-params = "poolMinSessions=50&poolMaxSessions=1000&poolWaitTimeout=360s&poolSessionMaxLifetime=2h&poolSessionTimeout=2h&poolIncrement=30&timezone=Local&connect_timeout=15"
# All/Full/CSV 模式内置 Date/Timestamp/Interval Year/Day 数据类型格式化
# Date 'yyyy-mm-dd hh24:mi:ss'
# Timestamp 'yyyy-mm-dd hh24:mi:ss.ffx', x 根据 timestamp 精度格式化, 如果超过 6, 按精度 6 格式化字符
# Interval Year/Day 数据字符 TO_CHAR 格式化
session-params = []
# 配置 oracle 迁移 schema（gather 阶段可设置可不设置，不设置则表示 gather 库内所有 schema，其他阶段必须设置）
schema-name = "marvin"
# 源端迁移任务表（只用于 prepare/reverse/check/all/full 阶段，gather 阶段不适用，gather 只适用于 schema 级别）
# include-table 和 exclude-table 不能同时配置，两者只能配置一个,如果两个都没配置则 Schema 内表全迁移
# include-table 和 exclude-table 支持正则表达式以及通配符（tab_*/tab*）
include-table = ["marvin*","t1"]
exclude-table = []

# 只用于 prepare/reverse/check/all/full 阶段，gather 阶段不适用
[target]
# 数据库类型，only mysql/tidb
db-type = "tidb"
# 目标端连接串
username = "marvin"
password = "marvin"
host = "172.16.4.93"
port = 5500
# mysql 链接参数
connect-params = "charset=utf8mb4&multiStatements=true&parseTime=True&loc=Local"
# 目标端元数据库
# CREATE DATABASE IF NOT EXIST db_meta
meta-schema = "db_meta"
# 目标端导入 schema
schema-name = "steven"
# 表后缀可选项 - Only 适用于 TiDB
# TiDB 数据库全局生效（自动读取下游数据参数判定生效与否）：
# tidb_enable_clustered_index = on 全局聚簇索引，table-option 不生效
# tidb_enable_clustered_index = off 全局非聚簇索引，table-option 生效
# tidb_enable_clustered_index = int_only 受配置项 alter-primary-key 控制
# 如果 alter-primary-key = true，则所有主键默认使用非聚簇索引，table-option 生效
# 如果 alter-primary-key = false，除下整数类型的列构成的主键之外，table-option 生效
table-option = "SHARD_ROW_ID_BITS = 4 PRE_SPLIT_REGIONS = 4"


[log]
# 日志 level
log-level = "info"
# 日志文件路径
log-file = "./conf/transferdb.log"
# 每个日志文件保存的最大尺寸 单位：M
max-size = 128
# 文件最多保存多少天
max-days = 7
# 日志文件最多保存多少个备份
max-backups = 30